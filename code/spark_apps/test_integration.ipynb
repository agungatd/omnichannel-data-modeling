{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499dff30",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# In a Jupyter Notebook, you would run this in a single cell.\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# --- 1. Define Configuration Variables ---\n",
    "# These should match your docker-compose.yml setup.\n",
    "NESSIE_URI = \"http://nessie-catalog:19120/api/v1\"\n",
    "MINIO_URI = \"http://minio:9000\"\n",
    "MINIO_ACCESS_KEY = \"admin\"\n",
    "MINIO_SECRET_KEY = \"password\"\n",
    "LAKEHOUSE_BUCKET = \"lakehouse\"\n",
    "WAREHOUSE_PATH = f\"s3a://{LAKEHOUSE_BUCKET}/warehouse\"\n",
    "\n",
    "# --- 2. Define Spark Packages and Configurations ---\n",
    "# This is the most critical part for connecting Spark to the lakehouse stack.\n",
    "spark_packages = [\n",
    "    \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.2\",\n",
    "    \"org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.77.1\",\n",
    "    \"software.amazon.awssdk:bundle:2.17.230\", # AWS SDK for S3 access\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\" # Hadoop-AWS module for s3a filesystem\n",
    "]\n",
    "\n",
    "spark_conf = {\n",
    "    # -- General Spark Settings --\n",
    "    \"spark.jars.packages\": \",\".join(spark_packages),\n",
    "    \n",
    "    # -- SQL Extensions for Nessie and Iceberg --\n",
    "    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.SparkSqlExtensions,org.projectnessie.spark.extensions.NessieSparkSQLExtensions\",\n",
    "    \n",
    "    # -- Nessie Catalog Configuration --\n",
    "    \"spark.sql.catalog.nessie\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    \"spark.sql.catalog.nessie.catalog-impl\": \"org.apache.iceberg.nessie.NessieCatalog\",\n",
    "    \"spark.sql.catalog.nessie.warehouse\": WAREHOUSE_PATH,\n",
    "    \"spark.sql.catalog.nessie.uri\": NESSIE_URI,\n",
    "    \"spark.sql.catalog.nessie.ref\": \"main\", # Default branch in Nessie\n",
    "    \"spark.sql.catalog.nessie.authentication.type\": \"NONE\", # No auth for local setup\n",
    "\n",
    "    # -- S3/MinIO Configuration --\n",
    "    \"spark.hadoop.fs.s3a.endpoint\": MINIO_URI,\n",
    "    \"spark.hadoop.fs.s3a.access.key\": MINIO_ACCESS_KEY,\n",
    "    \"spark.hadoop.fs.s3a.secret.key\": MINIO_SECRET_KEY,\n",
    "    \"spark.hadoop.fs.s3a.path.style.access\": \"true\", # Required for MinIO\n",
    "    \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "}\n",
    "\n",
    "\n",
    "# --- 3. Build the Spark Session ---\n",
    "# Use a builder pattern to apply all configurations.\n",
    "builder = SparkSession.builder.appName(\"JupyterIntegrationTest\")\n",
    "\n",
    "for key, value in spark_conf.items():\n",
    "    builder = builder.config(key, value)\n",
    "\n",
    "print(\"Starting Spark Session...\")\n",
    "spark = builder.getOrCreate()\n",
    "print(\"Spark Session created successfully!\")\n",
    "\n",
    "\n",
    "# --- 4. Run the Integration Test ---\n",
    "# We will use Spark SQL to interact with the Nessie catalog.\n",
    "\n",
    "# Set the current catalog to 'nessie' so we don't have to prefix table names.\n",
    "spark.sql(\"USE nessie;\")\n",
    "\n",
    "# Create a new database/schema if it doesn't exist.\n",
    "DB_NAME = \"bronze\"\n",
    "print(f\"\\nCreating database '{DB_NAME}'...\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DB_NAME};\")\n",
    "spark.sql(f\"SHOW DATABASES;\").show()\n",
    "\n",
    "# Create a simple Iceberg table.\n",
    "TABLE_NAME = f\"{DB_NAME}.test_users\"\n",
    "print(f\"Creating table '{TABLE_NAME}'...\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
    "        id INT,\n",
    "        name STRING,\n",
    "        event_ts TIMESTAMP\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (months(event_ts));\n",
    "\"\"\")\n",
    "\n",
    "# Insert some data into the table.\n",
    "print(\"Inserting data...\")\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {TABLE_NAME} VALUES\n",
    "    (1, 'Alice', timestamp('2024-01-15T10:00:00')),\n",
    "    (2, 'Bob', timestamp('2024-02-20T12:30:00'));\n",
    "\"\"\")\n",
    "\n",
    "# Read the data back from the Iceberg table.\n",
    "print(f\"Reading data from '{TABLE_NAME}':\")\n",
    "df = spark.sql(f\"SELECT * FROM {TABLE_NAME};\")\n",
    "df.show()\n",
    "\n",
    "# Verify the data was written to MinIO by listing files in the warehouse.\n",
    "# Note: This uses a shell command via `os.system` for a quick check.\n",
    "# You can also browse MinIO UI at http://localhost:9001\n",
    "print(\"\\n--- Verification in MinIO ---\")\n",
    "print(f\"Check the MinIO bucket '{LAKEHOUSE_BUCKET}' in your browser.\")\n",
    "print(\"You should see a path like: warehouse/bronze/test_users/data/...\")\n",
    "\n",
    "\n",
    "# --- 5. Stop the Spark Session ---\n",
    "print(\"\\nStopping Spark session.\")\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
